{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_Improving_The_CV_Accuracy_Using_Convolution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohd-faizy/03_TensorFlow_In-Practice/blob/master/05_Improving_The_CV_Accuracy_Using_Convolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R6gHiH-I7uFa"
      },
      "source": [
        "# __Improving Computer Vision Accuracy using Convolutions__\n",
        "\n",
        "In the previous lessons you saw how to do fashion recognition using a __Deep Neural Network (DNN)__ containing __three layers__:\n",
        "\n",
        "\n",
        "- The $\\color{red}{\\textbf{Input-layer}}$ is in the __shape of the data__.\n",
        "- The $\\color{red}{\\textbf{Output-layer}}$ is in the shape of the __Desired output__ and a __hidden layer__.\n",
        "\n",
        "You experimented with the __impact of different sizes of hidden layer__, __number of training epochs__ etc on the _final accuracy_.\n",
        "\n",
        "For convenience, here's the entire code again. Run it and take a note of the test accuracy that is printed out at the end. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xcsRtq9OLorS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "c6a0b3e1-4b5f-4381-c1eb-6fc1e7d9c68a"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4936 - accuracy: 0.8281\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3701 - accuracy: 0.8662\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3345 - accuracy: 0.8775\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3120 - accuracy: 0.8849\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2934 - accuracy: 0.8921\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3451 - accuracy: 0.8773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zldEXSsF8Noz"
      },
      "source": [
        "> Your accuracy is probably about $89$% __on training__ and $87$% __on validation__ not bad... _But how do you make that even better?_ $\\rightarrow$ One way is to use something called $\\color{red}{\\textbf{Convolutions}}$ that is narrowing down the content of the image to _focus on specific, distinct, details_. \n",
        "\n",
        "- Take an array (usually `3x3 or 5x5`) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like __Edge detection__.\n",
        " - So, for example, `3x3 filter` that is defined for __Edge detection__ where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the __edges enhanced.__\n",
        "\n",
        "\n",
        "_This is perfect for computer vision_, because often it's features that can get highlighted like this that distinguish one item for another, and __the amount of information needed is then much less__...because you'll just train on the __highlighted features__.\n",
        "\n",
        "That's the concept of __Convolutional Neural Networks__.\n",
        " - Add some layers to do __Convolution__ before you have the __Dense layers__, and then the _Information going to the dense layers is more focussed_, and possibly _more accurate_.\n",
        "\n",
        "> __Run the below code__ $\\Rightarrow$ This is the same __Neural Network__ as earlier, but this time with $\\Rightarrow$ __Convolutional layers added first__. It will __take longer__, but look at the __impact on the accuracy__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C0tFgT1MMKi6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "0d87f512-9d7d-4c8b-cae5-804adf63f1dc"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Total Dataset contaibns 60'000 Training sample & 10'000 Test samples\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4437 - accuracy: 0.8426\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2965 - accuracy: 0.8921\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2482 - accuracy: 0.9083\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2171 - accuracy: 0.9193\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1891 - accuracy: 0.9284\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2481 - accuracy: 0.9113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uRLfZ0jt-fQI"
      },
      "source": [
        "> It's likely gone up to about $93$% __(approx.) on the training data__ and $91$% __on the validation data__. \n",
        "\n",
        "    That's Significant, and a step in the right direction!\n",
        "\n",
        "Try running it for more $epochs$ -- say about $20$, and explore the $results!$ But while the results might seem really good, _the validation results may actually go down_, due to something called $Overfitting$.\n",
        "\n",
        "> $overfitting$ occurs when the __Network learns the data from the training set really well__, but it's too specialised to only that data, and as a _result is less effective at seeing other data_.\n",
        " - For $Example$, if all your life you only saw $red$ shoes, then when you see a $red$ shoe you would be very good at identifying it, but $blue$ shoes might confuse you...and you know you should never mess with my $blue$.\n",
        "\n",
        "Then, look at the code again, and see, step by step how the $Convolutions$ were built:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RaLX5cgI_JDb"
      },
      "source": [
        "> __Step 1:__ is to _gather the data_. You'll notice that there's a bit of a change here in that the __training data needed to be reshaped__.\n",
        "-  That's because the __first convolution__ expects a __single tensor containing everything__, so instead of `60,000  28x28x1` items in a list, we have a `single 4D` list that is `60,000x28x28x1`, and the __same for the test images__ \n",
        " - If you don't do this, you'll get an $Error$ when training as _the Convolutions do not recognize the shape_. \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SS_W_INc_kJQ"
      },
      "source": [
        "__Next is to define your__ $model$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Now instead of the __input layer at the top__, we add a $Convolution$. The parameters are:\n",
        "\n",
        "1. The $NUMBER$ of _Convolutions_ you want to __generate__. Purely arbitrary, but good to start with something in the order of $32$\n",
        "2. The $SIZE$ of the Convolution, in this case a __3x3 grid__\n",
        "3. The $ACTIVATION$ function to use -- in this case we'll use $RELU$, which you might recall is the equivalent of returning x when x > 0, else returning 0\n",
        "4. In the first layer, the shape of the __Input data__.\n",
        "\n",
        "> __Convolution__ Followed by $\\Rightarrow$ __MaxPooling layer__ which is then designed to _compress the image_, while maintaining the content of the features that were highlighted by the convlution.\n",
        " -  By Specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. the idea is that it creates a __2x2 array of pixels__, and picks the __biggest one__, thus turning $4$ pixels into $1$ . It repeats this across the image, and in so doing halves the number of $Horizontal$, and halves the number of $Vertical$ pixels, effectively reducing the image by $25$%.\n",
        "\n",
        "You can call `model.summary()` to see the _size and shape_ of the network, and you'll notice that after every $MaxPooling$ layer, the image size is $reduced$ in this way. \n",
        "\n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RMorM6daADjA"
      },
      "source": [
        "_Add another_ $Convolution$\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b1-x-kZF4_tC"
      },
      "source": [
        "Now $flatten$ the output. After this you'll just have the same $DNN$ structure as the non convolutional version\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Flatten(),\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qPtqR23uASjX"
      },
      "source": [
        "The same $128$ dense layers, and $10$ output layers as in the pre-convolution example:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C0GSsjUhAaSj"
      },
      "source": [
        "Now `compile` the model, call the `fit` method to do the training, and `evaluate` the _loss and accuracy_ from the test set.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IXx_LX3SAlFs"
      },
      "source": [
        "# __Visualizing the Convolutions and Pooling__\n",
        "\n",
        "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first $100$ labels in the test set, and you can see that the ones at __index 0, index 23 and index 28__ are all the __same value (9)__.__They're all shoes.__ \n",
        "\n",
        "Let's take a look at the result of running the $Convolution$ on each, and you'll begin to see common features between them emerge.\n",
        "\n",
        "\n",
        "Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f-6nX4QsOku6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2789d51a-894a-4cec-a223-1f314d003b2d"
      },
      "source": [
        "print(test_labels[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
            " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
            " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9FGsHhv6JvDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "d57a4767-8463-4608-8005-358fd9f1096c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,4)\n",
        "FIRST_IMAGE=0\n",
        "SECOND_IMAGE=7\n",
        "THIRD_IMAGE=26\n",
        "CONVOLUTION_NUMBER = 1\n",
        "\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  \n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8KVPZqgHo5Ux"
      },
      "source": [
        "## __EXERCISES__\n",
        "\n",
        "1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.\n",
        "\n",
        "2. Remove the final Convolution. What impact will this have on accuracy or training time?\n",
        "\n",
        "3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.\n",
        "\n",
        "4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. \n",
        "\n",
        "5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZpYRidBXpBPM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "4f29f584-ce66-4a31-b24a-1b3b92f0474b"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1870 - accuracy: 0.9450\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0616 - accuracy: 0.9811\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0384 - accuracy: 0.9882\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0262 - accuracy: 0.9918\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0175 - accuracy: 0.9945\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0475 - accuracy: 0.9850\n",
            "0.9850000143051147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYqDeq_zsDMG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# __Ex-1__\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "Epoch 5/5\n",
        "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0175 - accuracy: 0.9945\n",
        "313/313 [==============================] - 1s 2ms/step - loss: 0.0475 - accuracy: 0.9850\n",
        "0.9850000143051147\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSNgP17NqKfs",
        "colab_type": "text"
      },
      "source": [
        "# __Ex-2__\n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Epoch 5/5\n",
        "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0139 - accuracy: 0.9956\n",
        "313/313 [==============================] - 1s 2ms/step - loss: 0.0452 - accuracy: 0.9871\n",
        "0.9871000051498413\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OUkI--wqd4N",
        "colab_type": "text"
      },
      "source": [
        "# __Ex-3__\n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Epoch 5/5\n",
        "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0160 - accuracy: 0.9945\n",
        "313/313 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9893\n",
        "0.989300012588501\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsTkWeREq69P",
        "colab_type": "text"
      },
      "source": [
        "# __Ex-4__\n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Epoch 5/5\n",
        "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2934 - accuracy: 0.8921\n",
        "313/313 [==============================] - 1s 2ms/step - loss: 0.3451 - accuracy: 0.8773\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJTLUUWtt5jw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "6cbab07a-466b-4d45-95a7-62b3cea6d6a2"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "# Defining the callback class\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.96):\n",
        "      print(\"\\nReached 96% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=10, callbacks=[callbacks])\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3831 - accuracy: 0.8651\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2562 - accuracy: 0.9070\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2065 - accuracy: 0.9250\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1746 - accuracy: 0.9365\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1458 - accuracy: 0.9457\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1207 - accuracy: 0.9557\n",
            "Epoch 7/10\n",
            "1866/1875 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9620\n",
            "Reached 96% accuracy so cancelling training!\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1011 - accuracy: 0.9619\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2714 - accuracy: 0.9156\n",
            "0.9156000018119812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "531O_p0pyUvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "9606d786-e964-4308-f21d-aeea740ce3a5"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3785 - accuracy: 0.8645\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2568 - accuracy: 0.9065\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2102 - accuracy: 0.9224\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1748 - accuracy: 0.9359\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1455 - accuracy: 0.9464\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1215 - accuracy: 0.9552\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1011 - accuracy: 0.9627\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0827 - accuracy: 0.9697\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0695 - accuracy: 0.9747\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0564 - accuracy: 0.9794\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3262 - accuracy: 0.9161\n",
            "0.916100025177002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "153VHf4HzS0n",
        "colab_type": "text"
      },
      "source": [
        "__With Callback__\n",
        "\n",
        "```\n",
        "Epoch 7/10\n",
        "1866/1875 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9620\n",
        "```\n",
        "__Without Using Callback__\n",
        "\n",
        "```\n",
        "Epoch 7/10\n",
        "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1011 - accuracy: 0.9627\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}